View(data)
# R code for Regression 1.4: Prediction Analysis
# Remove all vars
rm(list=ls())
# Set directory(this should be set to user's own directory)
setwd("")
# Load data
load(file="pay.discrimination.Rdata")
# See variables in the dataset
class(data)
str(data)
# Dimensions of the dataset
dim(data)
# Table of means of each var
stats <- as.matrix(apply(data, 2, mean))
# Load xtable package to create latex tables
library(xtable)
# Assign colname to table stats
colnames(stats) = c("average")
xtable(stats)
####################  Linear and Quadratic specifications ##############################
# Wage linear regression
fmla1     <-  wage ~ female + sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Run Linear Specification and compute MSE and R^2
full.fit1 <-  lm(fmla1, data=data)
fit1      <-  summary(full.fit1)
R2.1      <-  fit1$r.squared
R2.adj1   <-  fit1$adj.r.squared
n1        <-  length(fit1$res)
p1        <-  fit1$df[1]
MSE.adj1  <-  (n1/(n1-p1))*mean(fit1$res^2)
# Linear regression: Quadratic specification
fmla2     <- wage ~  female + (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
# Run Quadratic Specification and compute MSE and R^2
full.fit2 <- lm(fmla2, data=data)
fit2      <- summary(full.fit2)
R2.2      <- fit2$r.squared
R2.adj2   <- fit2$adj.r.squared
n2        <- length(fit2$res)
p2        <- fit2$df[1]
MSE.adj2  <- (n2/(n2-p2))*mean(fit2$res^2)
# Summary of linear and quadratic specifications
table1     <- matrix(0, 2, 4)
table1[1,] <- c(p1, R2.1, R2.adj1, MSE.adj1)
table1[2,] <- c(p2, R2.2, R2.adj2, MSE.adj2)
# Print Regresssion Results
colnames(table1) <- c("p", "R^2", "R^2 adj", "MSE adj")
rownames(table1) <- c("basic reg", "flex reg")
####################  Linear and Quadratic specifications with Sample Splitting ##############################
# set random number generator
set.seed(123)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run linear specification and compute MSE and R^2 for the test sample
full.fit1  <- lm(fmla1, data=data[train,])
yhat.fit1  <- predict(full.fit1, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit1   <- summary(lm((y.test-yhat.fit1)^2~1))$coef[1]
R2.fit1    <- 1- MSE.fit1/var(y.test)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run quadratic specification and compute MSE and R^2 for the test sample
full.fit2  <- lm(fmla2, data=data[train,])
yhat.fit2  <- predict(full.fit2, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit2   <- summary(lm((y.test-yhat.fit2)^2~1))$coef[1]
R2.fit2    <- 1- MSE.fit2/var(y.test)
# Create result table
table2      <- matrix(0, 2, 3)
table2[1,]  <- c(p1, R2.fit1, MSE.fit1)
table2[2,]  <- c(p2, R2.fit2, MSE.fit2)
# Give Columns and Row Names
colnames(table2)  <- c("p ", "R2 test", "MSE test")
rownames(table2)  <- c("basic reg", "flex reg")
# Print Results
print(table1,digits=4)
print(table2,digits=4)
setwd("~/Dropbox (MIT)/Current/RA/MITReg/CASES/case1")
# R code for Regression 1.4: Prediction Analysis
# Remove all vars
rm(list=ls())
# Set directory(this should be set to user's own directory)
setwd("")
# Load data
load(file="pay.discrimination.Rdata")
# See variables in the dataset
class(data)
str(data)
# Dimensions of the dataset
dim(data)
# Table of means of each var
stats <- as.matrix(apply(data, 2, mean))
# Load xtable package to create latex tables
library(xtable)
# Assign colname to table stats
colnames(stats) = c("average")
xtable(stats)
####################  Linear and Quadratic specifications ##############################
# Wage linear regression
fmla1     <-  wage ~ female + sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Run Linear Specification and compute MSE and R^2
full.fit1 <-  lm(fmla1, data=data)
fit1      <-  summary(full.fit1)
R2.1      <-  fit1$r.squared
R2.adj1   <-  fit1$adj.r.squared
n1        <-  length(fit1$res)
p1        <-  fit1$df[1]
MSE.adj1  <-  (n1/(n1-p1))*mean(fit1$res^2)
# Linear regression: Quadratic specification
fmla2     <- wage ~  female + (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
# Run Quadratic Specification and compute MSE and R^2
full.fit2 <- lm(fmla2, data=data)
fit2      <- summary(full.fit2)
R2.2      <- fit2$r.squared
R2.adj2   <- fit2$adj.r.squared
n2        <- length(fit2$res)
p2        <- fit2$df[1]
MSE.adj2  <- (n2/(n2-p2))*mean(fit2$res^2)
# Summary of linear and quadratic specifications
table1     <- matrix(0, 2, 4)
table1[1,] <- c(p1, R2.1, R2.adj1, MSE.adj1)
table1[2,] <- c(p2, R2.2, R2.adj2, MSE.adj2)
# Print Regresssion Results
colnames(table1) <- c("p", "R^2", "R^2 adj", "MSE adj")
rownames(table1) <- c("basic reg", "flex reg")
####################  Linear and Quadratic specifications with Sample Splitting ##############################
# set random number generator
set.seed(123)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run linear specification and compute MSE and R^2 for the test sample
full.fit1  <- lm(fmla1, data=data[train,])
yhat.fit1  <- predict(full.fit1, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit1   <- summary(lm((y.test-yhat.fit1)^2~1))$coef[1]
R2.fit1    <- 1- MSE.fit1/var(y.test)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run quadratic specification and compute MSE and R^2 for the test sample
full.fit2  <- lm(fmla2, data=data[train,])
yhat.fit2  <- predict(full.fit2, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit2   <- summary(lm((y.test-yhat.fit2)^2~1))$coef[1]
R2.fit2    <- 1- MSE.fit2/var(y.test)
# Create result table
table2      <- matrix(0, 2, 3)
table2[1,]  <- c(p1, R2.fit1, MSE.fit1)
table2[2,]  <- c(p2, R2.fit2, MSE.fit2)
# Give Columns and Row Names
colnames(table2)  <- c("p ", "R2 test", "MSE test")
rownames(table2)  <- c("basic reg", "flex reg")
# Print Results
print(table1,digits=4)
print(table2,digits=4)
setwd("~/Dropbox (MIT)/Current/RA/MITReg/CASES/case3")
# R code for Regression 1.6: Gender Waage Gap
# Load hdm library
library(hdm)
# Load Dataset and see variables and the number of observations.
load(file="growth.Rdata")
dim(growth)
str(growth)
# Get variable names
varnames= colnames(growth)
# Set Directory
setwd("/Users/VC/Dropbox/MITReg/CASES/case2")
# Extract the names of control and treatment variables from varnames
xnames     <- varnames[-c(1,2,3)]     # names of X variables
dandxnames <- varnames[-c(1,2)]       # names of D and X variables
# create formulas by pasting names (this saves typing times)
fmla      <-  as.formula(paste("Outcome ~ ", paste(dandxnames, collapse= "+")))
full.fit  <-  lm(fmla, data=growth)
fmla.y    <-  as.formula(paste("Outcome ~ ", paste(xnames, collapse= "+")))
fmla.d    <-  as.formula(paste("gdpsh465~ ", paste(xnames, collapse= "+")))
# partial d and y by linear regression
rY       <- rlasso(fmla.y, data =growth)$res
rD       <- rlasso(fmla.d, data =growth)$res
# regress partialed out Y on partialed out D
partial.fit.lasso <- lm(rY~rD-1)
# create table to store results
table      <- matrix(0, 2, 2)
table[1,]  <- summary(full.fit)$coef["gdpsh465",1:2]
table[2,]  <- summary(partial.fit.lasso)$coef[1,1:2]
# give column and row names
colnames(table) <- names(summary(full.fit)$coef["gdpsh465",])[1:2]
rownames(table) <- c("Least Squares", "Partialling-out via lasso")
# print results
print(table, digits=2)
setwd("~/Dropbox (MIT)/Current/RA/MITReg/CASES/case5")
################################# Load Libraries #################################
library(foreign);
library(quantreg);
library(mnormt);
library(gbm);
library(glmnet);
library(MASS);
library(rpart);
library(sandwich);
library(hdm);
library(randomForest);
library(xtable)
library(nnet)
library(neuralnet)
library(caret)
library(matrixStats)
library(devtools)
library(plyr)
#################################  Loading functions and Data ########################
# Clear variables.
rm(list = ls())
# Set Random Number Generator.
set.seed(1)
# Set Directory
setwd("/Users/VC/Dropbox/MITReg/CASES/case5")
# Load Functions from External Files.
source("Cond-comp.R")
source("Functions.R")
# Load Data
data <- read.csv("gun_clean.csv")
# Table for Storing Results
table <- matrix(0,24,1)
#################################  Find Variable Names from Dataset ########################
varlist <- function (df=NULL,type=c("numeric","factor","character"), pattern="", exclude=NULL) {
vars <- character(0)
if (any(type %in% "numeric")) {
vars <- c(vars,names(df)[sapply(df,is.numeric)])
}
if (any(type %in% "factor")) {
vars <- c(vars,names(df)[sapply(df,is.factor)])
}
if (any(type %in% "character")) {
vars <- c(vars,names(df)[sapply(df,is.character)])
}
vars[(!vars %in% exclude) & grepl(vars,pattern=pattern)]
}
################################# Create Variables ###############################
# Dummy Variables for Year and County Fixed Effects
fixed  <- grep("X_Jfips", names(data), value=TRUE, fixed=TRUE)
year   <- varlist(data, pattern="X_Tyear")
# Census Control Variables
census     <- NULL
census_var <- c("^AGE", "^BN", "^BP", "^BZ", "^ED", "^EL","^HI", "^HS", "^INC", "^LF", "^LN", "^PI", "^PO", "^PP", "^PV", "^SPR", "^VS")
for(i in 1:length(census_var)){
census  <- append(census, varlist(data, pattern=census_var[i]))
}
# Treatment Variable
d     <- "logfssl"
# Outcome Variable
y     <- "logghomr"
# Other Control Variables
X1    <- c("logrobr", "logburg", "burg_missing", "robrate_missing")
X2    <- c("newblack", "newfhh", "newmove", "newdens", "newmal")
#################################  Partial out Fixed Effects ########################
# New Dataset for Partiled-out Variables
rdata    <- as.data.frame(data$CountyCode)
colnames(rdata) <- "CountyCode"
# Variables to be Partialled-out
varlist <- c(y, d,X1, X2, census)
# Partial out Variables in varlist from year and county fixed effect
for(i in 1:length(varlist)){
form <- as.formula(paste(varlist[i], "~", paste(paste(year,collapse="+"),  paste(fixed,collapse="+"), sep="+")))
rdata[, varlist[i]] <- lm(form, data)$residuals
}
############################# Linear Regression #############################
form1 <- as.formula(paste(y, "~", d ))
form2 <- as.formula(paste(y, "~", paste(d, paste(X1,collapse="+"), paste(X2,collapse="+"), paste(census,collapse="+"),   sep="+")))
table[1:2,1] <- summary(lm(form1, rdata))$coef[2,1:2]
table[3:4,1] <- summary(lm(form2, rdata))$coef[2,1:2]
######################## Double Machine Learning Methods ########################
# Outcome Variable
y.name      <-  y;
# Treatment Indicator
d.name      <- d;
# Controls
x.name      <- paste(paste(X1,collapse="+"),  paste(X2,collapse="+"), paste(census,collapse="+"), sep="+") # use this for tree-based methods like forests and boosted trees
# Method names
method      <- c("RLasso","PostRLasso", "Forest", "Boosting", "Trees", "Lasso", "Ridge", "Elnet", "Nnet")
# A Function that Returns Coefficients Estimated by Double Machine Learning Methods
#argument list:
#    1 : data
#    2 : outcome variable
#    3 : treatment variable
#    4 : control variables for tree-based methods
#    5 : control variables for linear models(flexible specification) - 4 and 5 are the same for this example
#    6 : method names
#    7 : number of split
#    8 : partially linear model
res <- DoubleML(rdata, y.name, d.name, x.name, x.name, method=method, 2 ,"plinear")
table[5:6,1]   <-res[,1]
table[7:8,1]   <-res[,2]
table[9:10,1]  <-res[,3]
table[11:12,1] <-res[,4]
table[13:14,1] <-res[,5]
table[15:16,1] <-res[,6]
table[17:18,1] <-res[,7]
table[19:20,1] <-res[,8]
table[21:22,1] <-res[,9]
table[23:24,1] <-res[,10]
################################# Print Results #################################
colnames(table) <- c("Gun")
rownames(table) <- c("Baseline1", "se(Baseline1)", "Baseline2", "sd(Baseline2)", "RLasso","sd(RLasso)", "PostRLasso", "sd(PostRLasso)",  "Forest", "sd(Forest)",
"Boosting", "sd(Boosting)", "Trees", "sd(Trees)", "Lasso", "sd(Lasso)", "Ridge", "sd(Ridge)", "Elnet", "sd(Elnet)", "Nnet", "sd(Nnet)", "Best", "sd(Best)")
print(table, digit=3)
setwd("~/Dropbox (MIT)/Current/RA/MITReg/CASES/case4")
##############################  variables ####################################
# wage               : wage, lwage
# gender             : sex
# race               : white, black, hisp
# education          : shs, hsg, scl, clg
# region             : mw, so, we
# union membership   : union
# veteran status     : vet
# city               : cent, ncent
# family size        : fam1, fam2, fam3
# having children    : child
# foreign born       : fborn
# citizenship        : cit
# school attandence  : school
# pension            : pens
# firm size          : fsize10, fsize100
# health status      : health
# age                : age
# experience         : exp1, exp2, exp3, exp4
# occupation         : occ(factor with 456 levels), occ2(factor with 22 levels-aggregated)
# industry           : ind(factor with 257 levels), ind2(factor with 23 levels-aggregated)
###############################################################################
# Clear workspace
rm(list=ls())
# Load Necessary Libraries
library(hdm)
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
library(rpart.plot)
library(xtable)
# Set Directory
setwd("/Users/VC/Dropbox/MITReg/CASES/case4")
# Load Data
load(file="wage2015.Rdata")
# Split Data into Training and Testing Sample
set.seed(1)
# Set of indices for training
training <- sample(nrow(data), nrow(data)*(1/2), replace=FALSE)
# training data
datause <- data[training,]
# test data
dataout <- data[-training,]
# Linear Control Variables. Use This Control Variables for Tree Based Machine Learning Methods.
x <- "sex+white+black+hisp+shs+hsg+scl+clg+mw+so+we+union+vet+cent+ncent+fam1+fam2+fam3+child+fborn+cit+school+pens+fsize10+fsize100+health+age+exp1+occ2+ind2"
# Quadratic Control Variables(Flexible Specification). Use This Control Variables for Linear methods
xL <- "(sex+white+black+hisp+shs+hsg+scl+clg+mw+so+we+union+vet+cent+ncent+fam1+fam2+fam3+child+fborn+cit+school+pens+fsize10+fsize100+health+age+exp1+exp2+exp3+exp4+occ2+ind2)^2"
# outcome variable log wage
y  <- "lwage"
# Linear Model: Quadratic (Large) Specification
formL <- as.formula(paste(y, "~", xL))
# Linear Model: Linear Specification
form  <- as.formula(paste(y, "~", x))
# "-1" : do not include constant in linear model
# x,y TRUE: return matrix of covariates/vector of outcomes
# Run these linear regression to use their outcome variables (fituse$y) and covariates(fituse$y)
# a trick to extract x and y variables from a formula.
fituseL    <- lm(paste(y, "~", xL, "-1"), datause, x=TRUE, y=TRUE)
fitoutL    <- lm(paste(y, "~", xL, "-1"), dataout, x=TRUE, y=TRUE)
fituse     <- lm(paste(y, "~", x, "-1"), datause, x=TRUE, y=TRUE)
fitout     <- lm(paste(y, "~", x, "-1"), dataout, x=TRUE, y=TRUE)
########################################## Train Models ##########################################
# linear regression
fit.lm      <- lm(form, datause)
fit.lm2     <- lm(formL, datause)
#lasso regression
# alpha=1: first norm(lasso), alpha=0: second norm (ridge)
# alpha = 0.5 : both penalties (elastic net)
#post = FALSE: do not re-run least squares on selected columns
#Lasso
fit.rlasso  <- rlasso(form, datause, post=FALSE)
#Post Lasso
fit.rlasso2 <- rlasso(form, datause, post=TRUE)
#CV-Lasso
fit.lasso   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=1)
#CV-Ridge
fit.ridge   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=0)
#CV-Elastic Net
fit.elnet   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=.5)
#Lasso Flexible
fit.rlassoL  <- rlasso(formL, datause, post=FALSE)
#Post-Lasso Flexible
fit.rlasso2L <- rlasso(formL, datause, post=TRUE)
#CV-Lasso Flexible
fit.lassoL   <- cv.glmnet(fituseL$x, fituseL$y, family="gaussian", alpha=1)
#CV-Ridge Flexible
fit.ridgeL   <- cv.glmnet(fituseL$x, fituseL$y, family="gaussian", alpha=0)
#CV-Elnet Flexible
fit.elnetL   <- cv.glmnet(fituseL$x, fituseL$y, family="gaussian", alpha=.5)
#Random Forest
fit.rf       <- randomForest(form, ntree=2000, nodesize=5, data=datause)
#Boosting Trees
fit.boost   <- gbm(form, data=datause, distribution= "gaussian", bag.fraction = .5, interaction.depth=2, n.trees=1000, shrinkage=.01)
best.boost  <- gbm.perf(fit.boost, plot.it = FALSE)
#Regression Trees
fit.trees   <- rpart(form, datause)
bestcp      <- trees$cptable[which.min(trees$cptable[,"xerror"]),"CP"]
fit.prunedtree <- prune(fit.trees,cp=bestcp)
#Neural Network
fit.nnet    <- nnet(form, datause, size=5,  maxit=1000, MaxNWts=100000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
########################################## Compute out-of-sample Predictions ##########################################
yhat.lm       <- predict(fit.lm, newdata=dataout)
yhat.lm2      <- predict(fit.lm2, newdata=dataout)
yhat.rlasso   <- predict(fit.rlasso, newdata=dataout)
yhat.rlasso2  <- predict(fit.rlasso2, newdata=dataout)
yhat.lasso    <- predict(fit.lasso, newx = fitout$x)
yhat.ridge    <- predict(fit.ridge, newx = fitout$x)
yhat.elnet    <- predict(fit.elnet, newx = fitout$x)
yhat.rlassoL  <- predict(fit.rlassoL, newdata=dataout)
yhat.rlasso2L <- predict(fit.rlasso2L, newdata=dataout)
yhat.lassoL   <- predict(fit.lassoL, newx = fitoutL$x)
yhat.ridgeL   <- predict(fit.ridgeL, newx = fitoutL$x)
yhat.elnetL   <- predict(fit.elnetL, newx = fitoutL$x)
yhat.rf       <- predict(fit.rf, newdata=dataout)
yhat.boost    <- predict(fit.boost, newdata=dataout, n.trees=best.boost)
yhat.pt       <- predict(fit.prunedtree,newdata=dataout)
yhat.nnet     <- predict(fit.nnet, newdata=dataout)
########################################## Compute Mean Squared Error for Each Model ##########################################
y.test       = dataout$lwage
MSE.lm       = summary(lm((y.test-yhat.lm)^2~1))$coef[1:2]
MSE.lm2      = summary(lm((y.test-yhat.lm2)^2~1))$coef[1:2]
MSE.rlasso   = summary(lm((y.test-yhat.rlasso)^2~1))$coef[1:2]
MSE.rlasso2  = summary(lm((y.test-yhat.rlasso2)^2~1))$coef[1:2]
MSE.lasso    = summary(lm((y.test-yhat.lasso)^2~1))$coef[1:2]
MSE.ridge    = summary(lm((y.test-yhat.ridge)^2~1))$coef[1:2]
MSE.elnet    = summary(lm((y.test-yhat.elnet)^2~1))$coef[1:2]
MSE.rlassoL  = summary(lm((y.test-yhat.rlassoL)^2~1))$coef[1:2]
MSE.rlasso2L = summary(lm((y.test-yhat.rlasso2L)^2~1))$coef[1:2]
MSE.lassoL   = summary(lm((y.test-yhat.lassoL)^2~1))$coef[1:2]
MSE.ridgeL   = summary(lm((y.test-yhat.ridgeL)^2~1))$coef[1:2]
MSE.elnetL   = summary(lm((y.test-yhat.elnetL)^2~1))$coef[1:2]
MSE.rf       = summary(lm((y.test-yhat.rf)^2~1))$coef[1:2]
MSE.boost    = summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
MSE.pt       = summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
MSE.nnet     = summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
########################################## Process Result into a Table ##########################################
table          <- matrix(0, 16, 3)
table[1,1:2]   <- MSE.lm
table[2,1:2]   <- MSE.lm2
table[3,1:2]   <- MSE.rlasso
table[4,1:2]   <- MSE.rlassoL
table[5,1:2]   <- MSE.rlasso2
table[6,1:2]   <- MSE.rlasso2L
table[7,1:2]   <- MSE.lasso
table[8,1:2]   <- MSE.lassoL
table[9,1:2]   <- MSE.ridge
table[10,1:2]  <- MSE.ridgeL
table[11,1:2]  <- MSE.elnet
table[12,1:2]  <- MSE.elnetL
table[13,1:2]  <- MSE.rf
table[14,1:2]  <- MSE.boost
table[15,1:2]  <- MSE.pt
table[16,1:2]  <- MSE.nnet
table[1,3]   <- 1-MSE.lm[1]/var(y.test)
table[2,3]   <- 1-MSE.lm2[1]/var(y.test)
table[3,3]   <- 1-MSE.rlasso[1]/var(y.test)
table[4,3]   <- 1-MSE.rlassoL[1]/var(y.test)
table[5,3]   <- 1-MSE.rlasso2[1]/var(y.test)
table[6,3]   <- 1-MSE.rlasso2L[1]/var(y.test)
table[7,3]   <- 1-MSE.lasso[1]/var(y.test)
table[8,3]   <- 1-MSE.lassoL[1]/var(y.test)
table[9,3]   <- 1-MSE.ridge[1]/var(y.test)
table[10,3]  <- 1-MSE.ridgeL[1]/var(y.test)
table[11,3]  <- 1-MSE.elnet[1]/var(y.test)
table[12,3]  <- 1-MSE.elnetL[1]/var(y.test)
table[13,3]  <- 1-MSE.rf[1]/var(y.test)
table[14,3]  <- 1-MSE.boost[1]/var(y.test)
table[15,3]  <- 1-MSE.pt[1]/var(y.test)
table[16,3]  <- 1-MSE.nnet[1]/var(y.test)
# Give column and row names
colnames(table)<- c("MSE", "S.E. for MSE", "R-squared")
rownames(table)<- c("Least Squares", "Least Squares(Flexible)", "Lasso", "Lasso(Flexible)", "Post-Lasso",  "Post-Lasso(Flexible)",
"Cross-Validated lasso", "Cross-Validated lasso(Flexible)", "Cross-Validated ridge", "Cross-Validated ridge(Flexible)", "Cross-Validated elnet", "Cross-Validated elnet(Flexible)",
"Random Forest","Boosted Trees", "Pruned Tree", "Neural Network")
#########################  Combinining Predictions/ Aggregations/ Ensemble Learning |Introduction  #################################
# Regress the outcome on predictions returned by each method : ordinary least squares
ens  <- lm(y.test~ yhat.lm+ yhat.rlasso+ yhat.elnet + yhat.rf+ yhat.pt +yhat.boost)
# Regress the outcome on predictions returned by each method : lasso, post=FALSE
ens2 <- rlasso(y.test~ yhat.lm+ yhat.rlasso+ yhat.elnet + yhat.rf+ yhat.pt + yhat.boost, post=FALSE)
# Mean Squared Error for Ensemble Learning
MSE.ens1  <- summary(lm((y.test-ens$fitted.values)^2~1))$coef[1:2]
MSE.ens2  <- summary(lm((y.test-predict(ens2))^2~1))$coef[1:2]
# Table of Results for Ensemble Learning
table2<- matrix(0, 7, 2)
table2[1,1]  <- ens$coefficients[1]
table2[2,1]  <- ens$coefficients[2]
table2[3,1]  <- ens$coefficients[3]
table2[4,1]  <- ens$coefficients[4]
table2[5,1]  <- ens$coefficients[5]
table2[6,1]  <- ens$coefficients[6]
table2[7,1]  <- ens$coefficients[7]
table2[1,2]  <- ens2$coefficients[1]
table2[2,2]  <- ens2$coefficients[2]
table2[3,2]  <- ens2$coefficients[3]
table2[4,2]  <- ens2$coefficients[4]
table2[5,2]  <- ens2$coefficients[5]
table2[6,2]  <- ens2$coefficients[6]
table2[7,2]  <- ens2$coefficients[7]
# Give Column and Row Names
colnames(table2)<- c("Weight(OLS)", "Weight(Rlasso)")
rownames(table2)<- c("Constant","Basic OLS","Lasso","Cross-Validated elnet", "Random Forest", "Pruned Tree","Boosted Trees")
# Print Results
print(table, digits=3)
print(table2, digits=3)
