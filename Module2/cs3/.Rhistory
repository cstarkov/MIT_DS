data= as.data.frame(cbind(Y,W))
paste(colnames(W),collapse="+")
length(Y)  # sample size
dim(W)[2]  # number of raw features
#- $Y$ is outcome
#- $W$ are raw predictors/features
# Split Data into Training and Testing Sample
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
fit.lm1<- lm(Y~W, data=data[training,])
fit.rlasso<- rlasso(W,Y, post=FALSE, subset=training,data=data)
fit.rlasso2<- rlasso(W,Y, post=TRUE, subset=training,data=data)
fit.lasso <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=1)
fit.ridge <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=0)
fit.elnet <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=.5)
fit.rf1= randomForest(W[training,], Y[training], mtry=dim(W)[2])
fit.rf2 = randomForest(W[training,], Y[training],mtry=dim(W)[2]/3)
fit.boost<- gbm.fit(W[training,-1],Y[training], distribution= "gaussian")
fit.trees<- rpart(Y~W,subset=training,data=data)
bestcp        <- trees$cptable[which.min(trees$cptable[,"xerror"]),"CP"]
fit.prunedtree          <- prune(fit.trees,cp=bestcp)
prp(fit.prunedtree)
#fit.nnet = nnet(Y~W, data=data[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
# Compute Out of Sample Performance
yhat.lm1<- predict(fit.lm1, newdata=data[-training,])
yhat.rlasso<- predict(fit.rlasso, newdata=data[-training,])
yhat.rlasso2<- predict(fit.rlasso2, newdata=data[-training,])
yhat.ridge <- predict(fit.ridge, newx =W[-training,])
yhat.elnet <- predict(fit.elnet, newx =W[-training,])
yhat.rf1<- predict(fit.rf1, newdata =W[-training,])
yhat.rf2<- predict(fit.rf2, newdata =W[-training,])
yhat.boost<- predict(fit.boost, newdata=data[-training,], n.trees=1000) #simple boosting
yhat.pt<- predict(fit.prunedtree,newdata=data[-training,])
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
y.test = Y[-training]
MSE.lm1= summary(lm((y.test-yhat.lm1)^2~1))$coef[1:2]
MSE.lm2= summary(lm((y.test-yhat.lm2)^2~1))$coef[1:2]
MSE.rlasso= summary(lm((y.test-yhat.rlasso)^2~1))$coef[1:2]
MSE.rlasso2= summary(lm((y.test-yhat.rlasso2)^2~1))$coef[1:2]
MSE.lasso = summary(lm((y.test-yhat.lasso)^2~1))$coef[1:2]
MSE.ridge = summary(lm((y.test-yhat.ridge)^2~1))$coef[1:2]
MSE.elnet = summary(lm((y.test-yhat.elnet)^2~1))$coef[1:2]
MSE.rf1 = summary(lm((y.test-yhat.rf2)^2~1))$coef[1:2]
MSE.rf2 = summary(lm((y.test-yhat.rf2)^2~1))$coef[1:2]
MSE.boost = summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
MSE.pt = summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
MSE.nnet = summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
MSE.nnet
MSE.pt
library(xtable)
table<- matrix(0, 12, 2)
table[1,]<- MSE.lm1
table[2,]<- MSE.lm2
table[3,]<- MSE.rlasso
table[4,]<- MSE.rlasso2
table[5,]<- MSE.lasso
table[6,]<- MSE.ridge
table[7,]<- MSE.elnet
table[8,]<- MSE.rf1
table[9,]<- MSE.rf2
table[10,]<- MSE.boost
table[11,]<- MSE.pt
table[12,]<- MSE.nnet
colnames(table)<- c("MSE", "S.E. for MSE")
rownames(table)<- c("Least Squares- Small Model", "Least Squares-Large Model", "Post-Lasso", "Lasso", "Cross-Validated lasso", "Cross-Validated ridge", "Cross-Validated elnet",  "Random Forest",  "Pruned Tree", "Neural Network")
tab<- xtable(table, digits =4)
print(tab, type="latex")
fit.lm1<- lm(Y~W, data=data[training,])
fit.rlasso<- rlasso(W,Y, post=FALSE, subset=training,data=data)
fit.rlasso2<- rlasso(W,Y, post=TRUE, subset=training,data=data)
fit.lasso <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=1)
fit.ridge <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=0)
fit.elnet <- cv.glmnet(W[training,], Y[training], family="gaussian", alpha=.5)
fit.rf1= randomForest(W[training,], Y[training], mtry=dim(W)[2])
fit.rf2 = randomForest(W[training,], Y[training],mtry=dim(W)[2]/3)
fit.boost<- gbm.fit(W[training,-1],Y[training], distribution= "gaussian")
fit.trees<- rpart(Y~W,subset=training,data=data)
bestcp        <- trees$cptable[which.min(trees$cptable[,"xerror"]),"CP"]
fit.prunedtree          <- prune(fit.trees,cp=bestcp)
prp(fit.prunedtree)
#fit.nnet = nnet(Y~W, data=data[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.lm1<- predict(fit.lm1, newdata=data[-training,])
yhat.lm2<- predict(fit.lm2, newdata=data[-training,] )
yhat.rlasso<- predict(fit.rlasso, newdata=data[-training,])
yhat.rlasso2<- predict(fit.rlasso2, newdata=data[-training,])
yhat.ridge <- predict(fit.ridge, newx =W[-training,])
yhat.elnet <- predict(fit.elnet, newx =W[-training,])
yhat.rf1<- predict(fit.rf1, newdata =W[-training,])
yhat.rf2<- predict(fit.rf2, newdata =W[-training,])
yhat.boost<- predict(fit.boost, newdata=data[-training,], n.trees=1000) #simple boosting
yhat.pt<- predict(fit.prunedtree,newdata=data[-training,])
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
y.test = Y[-training]
MSE.rlasso2= summary(lm((y.test-yhat.rlasso2)^2~1))$coef[1:2]
MSE.lm1= summary(lm((y.test-yhat.lm1)^2~1))$coef[1:2]
MSE.lm2= summary(lm((y.test-yhat.lm2)^2~1))$coef[1:2]
MSE.rlasso= summary(lm((y.test-yhat.rlasso)^2~1))$coef[1:2]
MSE.rlasso2= summary(lm((y.test-yhat.rlasso2)^2~1))$coef[1:2]
MSE.lasso = summary(lm((y.test-yhat.lasso)^2~1))$coef[1:2]
MSE.ridge = summary(lm((y.test-yhat.ridge)^2~1))$coef[1:2]
MSE.elnet = summary(lm((y.test-yhat.elnet)^2~1))$coef[1:2]
MSE.rf1 = summary(lm((y.test-yhat.rf2)^2~1))$coef[1:2]
MSE.rf2 = summary(lm((y.test-yhat.rf2)^2~1))$coef[1:2]
MSE.boost = summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
MSE.pt = summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
MSE.nnet = summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
MSE.boost
MSE.pt
MSE.rf2
MSE.rf1
MSE.nnet
MSE.ridge
MSE.lasso
MSE.rlasso
MSE.lm1
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
#datause = dataout[training,!(apply(datause, 2, var)==0)]
#dataout = data[-training,!(apply(dataout, 2, var)==0)]
#sum((apply(datause, 2, var)==0))
#sum((apply(dataout, 2, var)==0))
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause <- as.data.frame(scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout <- as.data.frame(scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
datause = data[training,]
dataout = data[-training,]
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])*(maxs[1]-mins[1])+mins[1]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
head(datause)
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
#datause = dataout[training,!(apply(datause, 2, var)==0)]
#dataout = data[-training,!(apply(dataout, 2, var)==0)]
#sum((apply(datause, 2, var)==0))
#sum((apply(dataout, 2, var)==0))
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause <- as.data.frame(scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout <- as.data.frame(scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])*(maxs[1]-mins[1])+mins[1]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
library(hdm)  #download most recent version from r-forge
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
library(rpart.plot)
data(EminentDomain)
Y = EminentDomain$logGDP$d
W = as.matrix(cbind(EminentDomain$logGDP$x, EminentDomain$logGDP$z))
data= as.data.frame(cbind(Y,W))
paste(colnames(W),collapse="+")
length(Y)  # sample size
dim(W)[2]  # number of raw features
#- $Y$ is outcome
#- $W$ are raw predictors/features
# Split Data into Training and Testing Sample
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause <- as.data.frame(scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout <- as.data.frame(scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])*(maxs[1]-mins[1])+mins[1]
y.test = Y[-training]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
plot(yhat.nnet, y.test)
plot(y.test)
head(data)
y
Y
yhat.nnet
plot(yhat.nnet, y.test)
(maxs[1]-mins[1])
mins[1]
predict(fit.nnet, newdata=data[-training,])
fit.nnet = nnet(datause[training,]~datause[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=dataout[-training,])*(maxs[1]-mins[1])+mins[1]
fit.nnet = nnet(datause[training,1]~datause[training,2:length(datause)], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
datause[training,1]
library(hdm)  #download most recent version from r-forge
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
library(rpart.plot)
data(EminentDomain)
Y = EminentDomain$logGDP$d
W = as.matrix(cbind(EminentDomain$logGDP$x, EminentDomain$logGDP$z))
data= as.data.frame(cbind(Y,W))
paste(colnames(W),collapse="+")
length(Y)  # sample size
dim(W)[2]  # number of raw features
#- $Y$ is outcome
#- $W$ are raw predictors/features
# Split Data into Training and Testing Sample
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
datause[,ind] <- as.data.frame(scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout[,ind] <- as.data.frame(scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
datause
sum(is.na(datause))
fit.nnet = nnet(datause[training,1]~datause[training,2:length(datause)], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
datause[training,2:length(datause)]
datause
fit.nnet = nnet(datause[,1]~datause[,2:length(datause)], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
2:length(datause)
datause[,2:length(datause)]
datause =  as.data.frame(datause)
dataout =  as.data.frame(dataout)
datause[,ind] <- (scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout[,ind] <- (scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
library(hdm)  #download most recent version from r-forge
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
library(rpart.plot)
data(EminentDomain)
Y = EminentDomain$logGDP$d
W = as.matrix(cbind(EminentDomain$logGDP$x, EminentDomain$logGDP$z))
data= as.data.frame(cbind(Y,W))
paste(colnames(W),collapse="+")
length(Y)  # sample size
dim(W)[2]  # number of raw features
#- $Y$ is outcome
#- $W$ are raw predictors/features
# Split Data into Training and Testing Sample
set.seed(1);
training <- sample(length(Y), length(Y)*(1/2), replace=FALSE)
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause =  as.data.frame(datause)
dataout =  as.data.frame(dataout)
datause[,ind] <- (scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout[,ind] <- (scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
datause[,2:length(datause)]
fit.nnet = nnet(datause[,1]~datause[,2:length(datause)], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
a = datause[,2:length(datause)]
fit.nnet = nnet(datause[,1]~a, size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(datause[,1]~datause[,-2], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(datause[,1]~datause[], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(datause[,1]~datause, size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
typeof(datause)
datause =  as.data.frame(datause)
dataout =  as.data.frame(dataout)
typeof(datause)
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause =  as.data.frame(datause)
dataout =  as.data.frame(dataout)
typeof(datause)
typeof(data)
typeof(W)
datause = data[training,]
typeof(datause)
typeof(data)
data= as.data.frame(cbind(Y,W))
typeof(data)
data= data.frame(cbind(Y,W))
typeof(data)
data= data.frame(Y,W)
type(data)
typeof(data)
typeof(X)
typeof(W)
class(data)
as.numeric(datause[,1])
typeof(datause[,2:length(datause)])
typeof(as.numeric(datause[,1]))
as.numeric(as.numeric(datause[,2:159]))
as.matrix(as.numeric(datause[,2:159]))
typeof(as.numeric(datause[,2:159]))
as.matrix(datause[,2:159])
fit.nnet = nnet(as.numeric(datause[,1])~as.matrix(datause[,2:159]), size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=dataout[-training,])*(maxs[1]-mins[1])+mins[1]
yhat.nnet<- predict(fit.nnet, newdata=as.matrix(dataout[-training,]))*(maxs[1]-mins[1])+mins[1]
yhat.nnet<- predict(fit.nnet, newdata=dataout)*(maxs[1]-mins[1])+mins[1]
fit.nnet = nnet(as.numeric(datause[,1])~as.matrix(datause[,2:159]), size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=dataout)*(maxs[1]-mins[1])+mins[1]
fit.nnet = nnet(as.numeric(datause[,1])~as.matrix(datause[,2:159]), size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=dataout)*(maxs[1]-mins[1])+mins[1]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
library(hdm)  #download most recent version from r-forge
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
library(rpart.plot)
data(EminentDomain)
Y = EminentDomain$logGDP$d
W = as.matrix(cbind(EminentDomain$logGDP$x, EminentDomain$logGDP$z))
data= as.data.frame(cbind(Y,W))
paste(colnames(W),collapse="+")
length(Y)  # sample size
dim(W)[2]  # number of raw features
datause = data[training,]
dataout = data[-training,]
maxs <- apply(datause, 2, max)
mins <- apply(datause, 2, min)
ind = !((maxs - mins)==0)
datause =  as.data.frame(datause)
dataout =  as.data.frame(dataout)
datause[,ind] <- (scale(datause[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
dataout[,ind] <- (scale(dataout[,ind], center = mins[ind], scale = maxs[ind] - mins[ind]))
fit.nnet = nnet(as.numeric(datause[,1])~as.matrix(datause[,2:159]), size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=dataout)*(maxs[1]-mins[1])+mins[1]
plot(yhat.nnet)
y.test = Y[-training]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
yhat.nnet<- predict(fit.nnet, newdata=dataout)*(maxs[1]-mins[1])+mins[1]
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
plot(yhat.nnet, y.test)
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
plot(yhat.nnet, y.test)
fit.nnet = nnet(Y~W, data=data[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=3000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(Y[training,], W[training,], size=5,  maxit=3000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(Y[training,], W[training,], size=5,  maxit=3000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(Y[training,], W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
class(Y)
class(data)
class(data[training,])
typeof(data[training,])
typeof(Y)
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
fit.nnet = nnet(Y~W, data=data[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
coef(fit.nnet)
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)
coef(fit.nnet)
attributes(nnet)
attributes(fit.nnet)
coef(fit.nnet)
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
yhat.nnet<- predict(fit.nnet)
fit.nnet = nnet(Y[training,]~W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
paste(colnames(W),collapse="+")
fit.nnet = nnet(Y[training,], W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(Y[training,], W[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
yhat.nnet<- predict(fit.nnet, newdata=data[-training,])
yhat.nnet<- predict(fit.nnet, newdata=W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.boost<- gbm.fit(W[training,-1],Y[training], distribution= "gaussian")
yhat.boost<- predict(fit.boost, newdata=data[-training,], n.trees=1000)
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
fit.trees<- rpart(Y~W,subset=training,data=data)
bestcp        <- trees$cptable[which.min(trees$cptable[,"xerror"]),"CP"]
fit.prunedtree          <- prune(fit.trees,cp=bestcp)
prp(fit.prunedtree)
yhat.pt<- predict(fit.prunedtree,newdata=data[-training,])
summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
yhat.pt<- predict(fit.prunedtree)
summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
yhat.pt<- predict(fit.prunedtree,newdata=W[-training,])
summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
yhat.nnet<- predict(fit.nnet, newdata=W[-training,])
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, newdata=W[-training,])
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
yhat.nnet<- predict(fit.nnet)
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet)
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
yhat.nnet<- predict(fit.nnet, W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.boost<- gbm.fit(W[training,-1],Y[training], distribution= "gaussian")
fit.boost<- gbm.fit(W[training,-1],Y[training], distribution= "gaussian")
yhat.boost<- predict(fit.boost, newdata=data[-training,], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
summary(lm((y.test-yhat.rlasso)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=data[-training,], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost, newdata=W[-training,-1], n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,  newdata=data[-training,],n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
yhat.boost<- predict(fit.boost,  newdata=data[-training,],n.trees=1000) #simple boosting
summary(lm((y.test-yhat.boost)^2~1))$coef[1:2]
fit.trees<- rpart(Y~W,subset=training,data=data)
bestcp        <- trees$cptable[which.min(trees$cptable[,"xerror"]),"CP"]
fit.prunedtree          <- prune(fit.trees,cp=bestcp)
prp(fit.prunedtree)
yhat.pt<- predict(fit.prunedtree,newdata=data[-training,])
summary(lm((y.test-yhat.pt)^2~1))$coef[1:2]
yhat.pt<- predict(fit.prunedtree,newdata=W[-training,])
attributes(fit.prunedtree)
fit.prunedtree#xlevels
fit.nnet = nnet(W[training,], Y[training,], size=5,  maxit=1000, decay=0.01, linout = TRUE, trace=FALSE)   # simple neural net fit
yhat.nnet<- predict(fit.nnet, W[-training,])
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
fit.lm1<- lm(Y~W, data=data[training,])
yhat.lm1<- predict(fit.lm1, newdata=data[-training,])
MSE.lm1= summary(lm((y.test-yhat.lm1)^2~1))$coef[1:2]
summary(lm((y.test-yhat.lm1)^2~1))$coef[1:2]
summary(lm((y.test-yhat.nnet)^2~1))$coef[1:2]
apply(ddata,1,function(x) sum(is.na(x)))
rm(list = ls())
library("hdm")
data(GrowthData)
growth= GrowthData
View(GrowthData)
save(growth, "growth.RData")
save(growth, file = "growth.RData")
setwd("~/Dropbox (MIT)/Current/RA/MITReg/CASES/case3")
save(growth, file = "growth.RData")
